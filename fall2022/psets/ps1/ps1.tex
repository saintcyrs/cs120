\documentclass[11pt]{article}
\usepackage{classTools}

\begin{document}

% To include a problem set header, use the psHeader command
\psHeader{1}{Wed Sep. 14, 2022 (11:59pm)}

Please review the Syllabus for information on the collaboration policy, grading scale, revisions, and late days.


\begin{enumerate}
    \item (Asymptotic Notation) 
    \begin{enumerate}
    \item (practice using asymptotic notation)
        Fill in the table below with ``T'' (for True) or ``F'' (for False) to indicate the relationship between $f$ and $g$. For example, if $f$ is $O(g)$, the first cell of the row should be ``T.'' \\
        \begin{table}[h!]
        \centering
        \bgroup
        \def\arraystretch{1.3}
        \begin{tabular}{||c | c || c | c | c | c | c ||}
         \hline
         $f$ & $g$ & $O$ & $o$ & $\Omega$ & $\omega$ & $\Theta$ \\
         \hline\hline
         $\sqrt{n}$ & $\log n$ & & & & & \\ \hline
         $n^{\sqrt{n}}$ & $n^{\log n}$ & & & & & \\ \hline
         $(\log {n^{120}})^2\sqrt{n}$ & $n$ & & & & & \\ \hline
         $\log(2^n)$ & $\log(e^n)$ & & & & & \\ \hline
         $2^{\sqrt{n}}$ & $n^{\log n}$ & & & & & \\ \hline
         $n^{(n \bmod 2)}$ & $n^{1/3}$ & & & & & \\ \hline
        \end{tabular}
        \egroup
        \end{table}
        Recall that, through CS120, all logarithms are base 2 unless otherwise specified. 
        
    \item  (rigorously reasoning about asymptotic notation)  
    For each of the following claims, either justify why the statement holds (for all $f$, $g$) or provide a counterexample. In all cases, take the domain of the functions $f$ and $g$ to be the natural numbers (rather than the positive reals), and assume $f(n), g(n)\geq 1$ for all sufficiently large $n$.
    \begin{itemize}
        \item For all positive constants $a$ and $b$, if $f(n) = \Theta(a^n)$ and $g(n) = \Theta(n^b)$, then $f(g(n)) = \Theta(a^{(n^b)})$.
        \item For all positive constants $a$ and $b$, if $f(n) = \Theta(a^n)$ and $g(n) = \Theta(n^b)$, then $g(f(n)) = \Theta((a^n)^b)$.
    \end{itemize}
  
    \end{enumerate}
    
    \newpage
    
    \item (Understanding computational problems and mathematical notation)\\\\
    Recall the definition of a {\em computational problem} from Lecture Notes 1.  

 
    Consider the following computational problem $\Pi=(\Inputs,\Outputs,f)$ and algorithm $\BC$ to solve it, where
    \begin{itemize}                                
    \item $\Inputs = \N\times\N\times \N$ 
    \item $\Outputs = \{(c_0,c_1,\ldots,c_{k-1}) : k,c_0,\ldots,c_{k-1}\in \N\}$
    \item $f(n,b,k) = \{ (c_0,c_1,\ldots,c_{k-1}) : n=c_0+c_1b+c_2b^2+\cdots+c_{k-1}b^{k-1}, \forall i\ 0\leq c_i< b\}.$ 
    \end{itemize}

    
\begin{algorithm}[H]
    \BC{$n,b,k$}\\
    {
%    \Input{$(n,b)$}
%    \Output{$(k,(c_0,c_1,\ldots,c_{k-1}))$}
    \lIf{$b<2$}{\Return{$\bot$}}
    \ForEach{$i=0,\ldots,k-1$}{
    $c_i = n \bmod b$\;
    $n = (n-c_i)/b$\;
    }
    \lIf{$n==0$}{\Return{$(c_0,c_1,\ldots,c_{k-1})$}}
    \lElse{\Return{$\bot$}}}
\end{algorithm}


\begin{enumerate}
\item Describe the computational problem $\Pi$ in words.  (You may find it useful to try some examples with $b=10$.)
\item For each possible input $x\in \Inputs$, what is $|f(x)|$?  Justify your answers in one or two sentences.


\item This problem does not refer to $\Pi$ and instead refers to an abstract computational problem. A one- or two-sentence explanation for this the questions should suffice:
%\begin{enumerate}
%\item 

Let $\Pi=(\Inputs,\Outputs,f)$ and $\Pi'=(\Inputs,\Outputs,f')$ be two computational problems.  Suppose that for all $x\in \Inputs$, we have $f(x)\subseteq f'(x)$.  Does it follow that every algorithm $A$ that solves $\Pi$ also solves $\Pi'$?    Does the answer change if we also require that $f(x)\neq \emptyset$ for every $x$? Justify your answers with a proof or a counterexample.
%\end{enumerate}


\end{enumerate}
\newpage

\item (Radix Sort) In the Sender--Receiver Exercise on Thursday September 8, you studied the sorting algorithm {\em Counting Sort}, generalized to arrays of key--value pairs, and proved that it has running time $O(n+U)$ when the keys are drawn from a universe of size $U$. In this problem you'll study {\em Radix Sort}, which improves the dependence on the universe size $U$ from linear to logarithmic.  Specifically, Radix Sort can achieve runtime $O(n+n(\log U)/(\log n))$, so it achieves runtime $O(n)$ whenever $U = n^{O(1)}$.  
Radix Sort is constructed by using Counting Sort as a subroutine several times, but on a smaller universe size $b$.
Crucially, Radix Sort uses the fact that Counting Sort can be implemented in a way that is {\em stable} in the sense that it preserves the order in the input array when the same key appears multiple times.  Here is pseudocode for Radix Sort:

\begin{algorithm}[H]
\RadixSort{$U,b,A$}\\
\Input{A universe size $U\in \N$, a base $b\in \N$ with $b\geq 2$, and an array $A=((K_0,V_0),\ldots,(K_{n-1},V_{n-1}))$, where each $K_i\in [U]$}
\Output{A valid sorting of $A$}
%$b=\min\{n,U\}$\;
$k=\lceil (\log U)/(\log b)\rceil$\;
\ForEach{$i=0,\ldots,n-1$}{
    $V_i' = \BC(K_i,b,k)$}
\ForEach{$j=0,\ldots,k-1$}{
    \ForEach{$i=0,\ldots,n-1$}{
    $K'_i = V'_i[j]$
    }
    $((K_0',(V_0,V'_0)),\ldots,(K_{n-1}',(V_{n-1},V'_{n-1}))) = \CountingSort(b,((K'_0,(V_0,V_0')),\ldots,(K'_{n-1},(V_{n-1},V'_{n-1})))$\;
}
\ForEach{$i=0,\ldots,n-1$}{
    $K_i = V'_i[0]+V'_i[1]\cdot b + V'_i[2]\cdot b^2+\cdots+V'_i[k-1]\cdot b^{k-1}$}
\Return{$((K_0,V_0),\ldots,(K_{n-1},V_{n-1}))$}
\caption{Radix Sort}
\end{algorithm}

(You can also read a description of Radix Sort in CLRS Section 8.3 for the case of sorting arrays of keys (without attached items) when $U$ and $b$ are powers of 2, albeit using different notation than us.)

        \begin{enumerate}
        
            \item (proving correctness of algorithms) Prove the correctness of \RadixSort\ (i.e. that it correctly solves the Sorting problem).  Where does your proof use the stability of \CountingSort?
      
            
            \item (analyzing runtime) Show that \RadixSort\ has runtime $O((n+b)\cdot \lceil \log_b U\rceil)$.  Set $b=\min\{n,U\}$ to obtain our desired runtime of $O(n+n(\log U)/(\log n))$.  (This runtime analysis is outlined in CLRS, but you'd need to adapt it to our notation and slightly more general setting.) 
            
            \item (implementing algorithms)
            Implement \RadixSort\ using the implementations of \CountingSort\ and \BC\ that we provide you in the GitHub repository. 
  
 \iffalse           
            Provide a more detailed pseudocode description of Radix Sort (for arrays of item-key pairs) following the above outline, using Counting Sort as a subroutine.  The inputs to Counting Sort are a natural number $U$ and an array $A$ of item-key pairs where the items are arbitrary objects and the keys are elements of $[U]$, so for each call your pseudocode makes to Counting Sort,  it should explicitly say what value of $U$ and what array $A$ is fed in. 
            Implement your version of Radix Sort in Python, using the implementation of Counting Sort that we will provide you in the github repository.
                The inputs to your implementation of Radix Sort should consist of an array of item-key pairs, the length $n$ of the array, the universe size $U$, and the
                base $b$.
\fi

            \item (experimentally evaluating algorithms) Run experiments to compare the expected runtime of \CountingSort, \RadixSort (with base $b=n$), and \MergeSort\ as $n$ and $U$ vary among powers of 2 with $1\leq n\leq 2^{16}$ and $1\leq U\leq 2^{20}$.  For each pair of $(n,U)$ values you consider, run multiple trials to estimate the expected runtime over random arrays where the keys are chosen uniformly and independently from $[U]$.  For each sufficiently large value of $n$, the asymptotic (albeit worst-case) runtime analyses suggest that \CountingSort\ should be the most efficient algorithm for small values of $U$, \MergeSort\ should be the most efficient algorithm for large values of $U$, and \RadixSort\ should be the most efficient somewhere in between.  Plot the transition points from \CountingSort to \RadixSort, and \RadixSort to \MergeSort\ on a $\log n$ vs. $\log U$ scale (as usual our logarithms are base 2).  Do the shapes of the resulting transition curves fit what you'd expect from the asymptotic theory?  Explain.  %(Note: depending on your implementation and your computer, the experiments may take hours, so be sure to carry them out well ahead of the deadline.)
            
            \textit{Note: We are expecting to see one (or more, if necessary) graphs that demonstrate, for every value of $n$, for which value of $U$ \RadixSort first outperforms \CountingSort and \MergeSort first outperforms \RadixSort. You should label the graphs appropriately (title, axis labels, etc.) and provide a caption, as well as an answer and explanation to the above question. Please look at the provided starter code for more information on generating random arrays, timing experiments, and graphing. Your implementation of RadixSort, as well as any code you write for experimentation and graphing need not be submitted.  Depending on your implementation, running the experiments could take anywhere from 15 minutes to a couple of hours, so don't leave them to the last minute!}   

          
        \end{enumerate}

\end{enumerate}


\end{document}